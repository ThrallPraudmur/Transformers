{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение BERT - модели для решения задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRATwwPa6fWp",
    "outputId": "e93e543a-b6e7-4e7f-8d0d-e34ef2db649b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8341708542713567"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from spacy.training import Example\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "comment_data = pd.read_excel('values_df.xlsx')\n",
    "comment_data = shuffle(comment_data)\n",
    "\n",
    "# Подготовка данных\n",
    "encoder = LabelEncoder()\n",
    "comment_data['class'] = encoder.fit_transform(comment_data['sentiment'])\n",
    "\n",
    "X = comment_data['text']\n",
    "y = comment_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify = comment_data['class'])\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42, stratify = y_test)\n",
    "\n",
    "# Сompute class frequencies\n",
    "# class_counts = Counter(y_train)\n",
    "# total_samples = len(y_train)\n",
    "# class_weights = torch.tensor([total_samples / (class_counts[i] * len(class_counts)) for i in range(len(class_counts))], dtype = torch.float)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels = 3)\n",
    "\n",
    "# Токенизация и преобразование комментариев\n",
    "train_encodings = tokenizer(list(X_train), truncation = True, padding = True)\n",
    "test_encodings = tokenizer(list(X_test), truncation = True, padding = True)\n",
    "\n",
    "# Создание тензоров pytorch для входных данных, dtype = torch.long\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                              torch.tensor(train_encodings['attention_mask']),\n",
    "                              torch.tensor(y_train.values),\n",
    "                            #   class_weights[y_train.values]\n",
    "                              )\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                             torch.tensor(test_encodings['attention_mask']),\n",
    "                             torch.tensor(y_test.values))\n",
    "\n",
    "# Создание dataloader для обучающего и тестового набора данных\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
    "\n",
    "# Установка GPU\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(10):\n",
    "  model.train()\n",
    "  for batch in train_loader:\n",
    "    input_ids, attention_mask, labels = (item.to(device) for item in batch)\n",
    "    # input_ids, attention_mask, labels = (item for item in batch)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "  for batch in test_loader:\n",
    "    input_ids, attention_mask, _ = (item.to(device) for item in batch)\n",
    "    # input_ids, attention_mask, _ = (item for item in batch)\n",
    "    outputs = model(input_ids, attention_mask = attention_mask)\n",
    "    _, predicted_labels = torch.max(outputs.logits, dim = 1)\n",
    "    predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qfsx6SXhN_5K",
    "outputId": "864406dd-63ce-4675-96ef-aa02cb4b82df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.94      0.88        82\n",
      "     neutral       0.94      0.52      0.67        29\n",
      "    positive       0.83      0.84      0.84        88\n",
      "\n",
      "    accuracy                           0.83       199\n",
      "   macro avg       0.86      0.77      0.79       199\n",
      "weighted avg       0.84      0.83      0.83       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = encoder.inverse_transform(y_test)\n",
    "predictions = encoder.inverse_transform(predictions)\n",
    "\n",
    "results_df = pd.DataFrame({'Comment': X_test, 'True_Label': y_test, 'Predicted_Label': predictions})\n",
    "print(classification_report(results_df['True_Label'], results_df['Predicted_Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ct3DvkblEdJZ",
    "outputId": "24cf9bf5-25e6-4916-8372-da0f289ad942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_model/tokenizer_config.json',\n",
       " 'saved_model/special_tokens_map.json',\n",
       " 'saved_model/vocab.txt',\n",
       " 'saved_model/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохранение модели и токенизатора\n",
    "model.save_pretrained('saved_model')\n",
    "tokenizer.save_pretrained('saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение BERT - модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение Енкодера\n",
    "name = '\\\\values_df.xlsx'\n",
    "comment_data = pd.read_excel(path + name)\n",
    "comment_data = shuffle(comment_data)\n",
    "encoder = LabelEncoder()\n",
    "comment_data['class'] = encoder.fit_transform(comment_data['sentiment'])\n",
    "\n",
    "name = '\\\\bert-further-train.xlsx'\n",
    "comment_data = pd.read_excel(path + name)\n",
    "comment_data['class'] = encoder.fit_transform(comment_data['sentiment'])\n",
    "\n",
    "X = comment_data['text']\n",
    "y = comment_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify = comment_data['class'])\n",
    "\n",
    "name = '\\\\bert_new'\n",
    "tokenizer = BertTokenizer.from_pretrained(path + name)\n",
    "model = BertForSequenceClassification.from_pretrained(path + name)\n",
    "\n",
    "# Токенизация и преобразование комментариев\n",
    "train_encodings = tokenizer(list(X_train), truncation = True, padding = True)\n",
    "test_encodings = tokenizer(list(X_test), truncation = True, padding = True)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids'], dtype=torch.long),\n",
    "                              torch.tensor(train_encodings['attention_mask'], dtype=torch.long),\n",
    "                              torch.tensor(y_train.values, dtype=torch.long),\n",
    "                            #   class_weights[y_train.values]\n",
    "                              )\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids'], dtype=torch.long),\n",
    "                             torch.tensor(test_encodings['attention_mask'], dtype=torch.long),\n",
    "                             torch.tensor(y_test.values, dtype=torch.long))\n",
    "# Создание dataloader для обучающего и тестового набора данных\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
    "\n",
    "# Установка GPU\n",
    "# device = torch.device('cuda')\n",
    "# model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(6): # Количество эпох архиважно, лучше не трогай\n",
    "  model.train()\n",
    "  for batch in train_loader:\n",
    "    # input_ids, attention_mask, labels = (item.to(device) for item in batch)\n",
    "    input_ids, attention_mask, labels = (item for item in batch)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "  for batch in test_loader:\n",
    "    # input_ids, attention_mask, _ = (item.to(device) for item in batch)\n",
    "    input_ids, attention_mask, _ = (item for item in batch)\n",
    "    outputs = model(input_ids, attention_mask = attention_mask)\n",
    "    _, predicted_labels = torch.max(outputs.logits, dim = 1)\n",
    "    predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели и токенизатора\n",
    "name = '\\\\bert_april'\n",
    "model.save_pretrained(path + name)\n",
    "tokenizer.save_pretrained(path + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '\\\\bert_april'\n",
    "model = BertForSequenceClassification.from_pretrained(path + name)\n",
    "tokenizer = BertTokenizer.from_pretrained(path + name)\n",
    "\n",
    "# Обучение Енкодера\n",
    "name = '\\\\values_df.xlsx'\n",
    "comment_data = pd.read_excel(path + name)\n",
    "comment_data = shuffle(comment_data)\n",
    "encoder = LabelEncoder()\n",
    "comment_data['class'] = encoder.fit_transform(comment_data['sentiment'])\n",
    "\n",
    "def get_prediction(text):\n",
    "    # Преобразование текста в токены\n",
    "    inputs = tokenizer(text, return_tensors = 'pt')\n",
    "    # Подача токенизированных данных в модель для получения предсказания\n",
    "    outputs = model(**inputs)\n",
    "    # Получение предсказанных классов в виде вероятностей\n",
    "    predicted_probabilities = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
    "    # Выбор индекса с максимальной вероятностью\n",
    "    predicted_class = torch.argmax(predicted_probabilities, dim = -1).item()\n",
    "    return encoder.inverse_transform([predicted_class])\n",
    "\n",
    "def evaluate_model(comment_data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(comment_data)\n",
    "\n",
    "    for index, row in comment_data[['text', 'sentiment']].iterrows():\n",
    "        text = row['text']\n",
    "        true_class = row['sentiment']\n",
    "        predicted_class = get_prediction(text)[0]\n",
    "\n",
    "        if predicted_class == true_class:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(comment_data[['text', 'sentiment']])\n",
    "accuracy # 0.9435215946843853"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение NLP - модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_md')\n",
    "ner = nlp.get_pipe('ner')\n",
    "ner.add_label('PROD') # 1\n",
    "\n",
    "data_to_train = pd.read_excel('train_data.xlsx')\n",
    "\n",
    "# ФУНКЦИЯ ПРИВЕДЕНИЯ ДАННЫХ К ТИПУ ТЕКСТ - СУЩНОСТЬ\n",
    "def get_train_data(data_to_train):\n",
    "    train_data = []\n",
    "    for index, row in data_to_train.iterrows():\n",
    "        text = row.text\n",
    "        selected_text = row.selected_text\n",
    "        entity_list = []\n",
    "        for elem in selected_text.split(sep = ','):\n",
    "            elem = elem.strip()\n",
    "            start = text.find(elem)\n",
    "            end = start + len(elem)\n",
    "            entity_list.append((start, end, 'PROD'))\n",
    "        train_data.append((text, {'entities': entity_list}))\n",
    "    return train_data\n",
    "\n",
    "# ФУНКЦИЯ ОЧИТКИ ДАННЫХ\n",
    "def datacleaner(text):\n",
    "    if type(text) = str:\n",
    "        words = []\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.is_alpha or (token.text == ',') or (token.text == '.') or (token.text == '+'):\n",
    "                words.append(token.text)\n",
    "        return ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# ПРЕДВАРИТЕЛЬНАЯ ОЧИСТКА ДАННЫХ\n",
    "data_to_train['text'] = data_to_train['text'].apply(datacleaner)\n",
    "\n",
    "# ПОЛУЧЕНИЕ ДАННЫХ\n",
    "train_data = get_train_data(data_to_train)\n",
    "\n",
    "# ОБУЧЕНИЕ\n",
    "for i in range(10):\n",
    "    train_data = shuffle(train_data)\n",
    "    losses = {}\n",
    "    try:\n",
    "        for text, annotations in train_data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.1, losses=losses)\n",
    "    except ValueError as e:\n",
    "        print(f'Ошибка при обновлении модели: {e}')\n",
    "    print('Эпоха', i, ':', losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '\\\\nlp_trained'\n",
    "nlp.to_disk(path + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование двух моделей на боевых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSacrYkrPIZW"
   },
   "outputs": [],
   "source": [
    "# Загрузка BERT-модели и токенизатора\n",
    "name = '\\\\bert_april'\n",
    "model = BertForSequenceClassification.from_pretrained(path + name)\n",
    "tokenizer = BertTokenizer.from_pretrained(path + name)\n",
    "\n",
    "# Загрузка NLP-модели\n",
    "nlp = spacy.load(path + name)\n",
    "\n",
    "def datacleaner(text):\n",
    "    words = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_alpha or (token.text == ',') or (token.text == '.') or (token.text == '+'):\n",
    "            words.append(token.text)\n",
    "    return ' '.join(words)\n",
    "\n",
    "deposit_list = ['деп', 'депоз', 'депозит', 'депы', 'депозы'] #Депозит\n",
    "credit_list = ['кик', 'вкл', 'нкл', 'кредитный', 'кредит', 'кредитоваться', 'овер', 'овера', 'оверу', 'овердрафт', #Кредит\n",
    "               'ВКЛ', 'кредитование', 'кд', 'линия', 'кк']\n",
    "bill_list = ['рс', 'счет', 'рко'] #РКО\n",
    "bg_list = ['гарантия', 'бг', 'Гарантия'] #Гарантия\n",
    "lease_list = ['лизинг'] #Лизинг\n",
    "ekviring_list = ['эквайрингу', 'эквайринг'] #Эквайринг\n",
    "product_list = ['продукт'] #Продукты\n",
    "secure_list = ['страховать', 'страхование', 'страх', 'страховка'] #Страхование\n",
    "salary_list = ['зп', 'зпп'] #ЗПП\n",
    "finance_list = ['финансирование'] #Финансирование\n",
    "factoring_list = ['факторинг', 'факторингу', 'фактор'] #Факторинг\n",
    "ved_list = ['вэд', 'вэду', 'вед', 'веду'] #ВЭД\n",
    "ostatok_list = ['остаток', 'остатки'] #Остатки\n",
    "\n",
    "def rebuild_col(column_dict):\n",
    "    newborn = {}\n",
    "    for key in column_dict.keys():\n",
    "        if (key in deposit_list) and ('Депозит' not in newborn.keys()):\n",
    "            newborn['Депозит'] = column_dict[key]\n",
    "        if (key in credit_list) and ('Кредит' not in newborn.keys()):\n",
    "                newborn['Кредит'] = column_dict[key]\n",
    "        if (key in bg_list) and ('Гарантия' not in newborn.keys()):\n",
    "            newborn['Гарантия'] = column_dict[key]\n",
    "        if (key in lease_list) and ('Лизинг' not in newborn.keys()):\n",
    "            newborn['Лизинг'] = column_dict[key]\n",
    "        if (key in ekviring_list) and ('Эквайринг' not in newborn.keys()):\n",
    "            newborn['Эквайринг'] = column_dict[key]\n",
    "        if (key in product_list) and ('Продукты' not in newborn.keys()):\n",
    "            newborn['Продукты'] = column_dict[key] # Ничего, Ничто, Желания ?\n",
    "        if (key in secure_list) and ('Страхование' not in newborn.keys()):\n",
    "            newborn['Страхование'] = column_dict[key]\n",
    "        # if key in finance_list:\n",
    "            # newborn['Финансирование'] = column_dict[key]\n",
    "        if (key in salary_list) and ('ЗПП' not in newborn.keys()):\n",
    "            newborn['ЗПП'] = column_dict[key]\n",
    "        if (key in factoring_list) and ('Факторинг' not in newborn.keys()):\n",
    "            newborn['Факторинг'] = column_dict[key]\n",
    "        if (key in bill_list) and ('РКО' not in newborn.keys()):\n",
    "            newborn['РКО'] = column_dict[key]\n",
    "        if (key in ved_list) and ('ВЭД' not in newborn.keys()):\n",
    "            newborn['ВЭД'] = column_dict[key]\n",
    "        if (key in ostatok_list) and ('Остатки' not in newborn.keys()):\n",
    "            newborn['Остатки'] = column_dict[key]\n",
    "    return newborn\n",
    "\n",
    "# ОСТАВЛЯЕМ ТОЛЬКО СУЩНОСТИ И ИХ ОКРУЖЕНИЕ\n",
    "def entlist_to_form(comment):\n",
    "    comment = comment.lower()\n",
    "    partlist = []\n",
    "    entlist = []\n",
    "    # Пустая строка\n",
    "    current_part = ''\n",
    "    doc = nlp(comment)\n",
    "    for token in doc:\n",
    "        if (token.text == ',') or (token.text == '.') or (token.text == '+'):\n",
    "            partlist.append(current_part)\n",
    "            current_part = ''\n",
    "        else:\n",
    "            current_part += token.text\n",
    "            current_part += ' '\n",
    "    # Если строка НЕ пустая, добавим часть предложения, в список\n",
    "    if current_part:\n",
    "        partlist.append(current_part)\n",
    "    # Пройдём по ранее заготовленному списку partlist\n",
    "    for part in partlist:\n",
    "        for ent in doc.ents: # doc = nlp(text)\n",
    "            if ent.label_ == 'PROD' and ent.text in part:\n",
    "                entlist.append({ent.lemma_: part}) #ПРИВОДИМ К ФОРМАТУ СУЩНОСТЬ-ЗНАЧЕНИЕ\n",
    "    return entlist\n",
    "\n",
    "def get_prediction(text):\n",
    "    # Преобразование текста в токены\n",
    "    inputs = tokenizer(text, return_tensors = 'pt')\n",
    "    # Подача токенизированных данных в модель для получения предсказания\n",
    "    outputs = model(**inputs)\n",
    "    # Получение предсказанных классов в виде вероятностей\n",
    "    predicted_probabilities = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
    "    # Выбор индекса с максимальной вероятностью\n",
    "    predicted_class = torch.argmax(predicted_probabilities, dim = -1).item()\n",
    "    return encoder.inverse_transform([predicted_class])\n",
    "\n",
    "def form_sentiment_data(comment_data):\n",
    "    sentiment_dict = {}\n",
    "    # На вход подаём список из словарей\n",
    "    for elem in comment_data:\n",
    "        # Проходим по каждому из словарей списка и формируем sentiment_dict\n",
    "        for key, value in elem.items():\n",
    "            if key not in sentiment_dict:\n",
    "                sentiment_dict[key] = get_prediction(value)\n",
    "    return sentiment_dict\n",
    "\n",
    "def form_comment_data(comment_data):\n",
    "    comment_dict = {}\n",
    "    # На вход подаём список из словарей\n",
    "    for elem in comment_data:\n",
    "        # Проходим по каждому из словарей списка и формируем уже comment_dict\n",
    "        for key, value in elem.items():\n",
    "            if key not in comment_dict: # Если ключ ещё не добавлен, то добавляем -> сохраняем первое упоминание\n",
    "                comment_dict[key] = value\n",
    "    return comment_dict\n",
    "\n",
    "test_data = result[['ИНН', 'Дата изменения', 'Описание']].copy()\n",
    "\n",
    "# ПРЕДОБРАБОТКА КОММЕНТАРИЯ ИЗ БАЗЫ\n",
    "test_data = test_data[~test_data['Описание'].isna()]\n",
    "\n",
    "# Обучение Енкодера\n",
    "name = '\\\\values_df.xlsx'\n",
    "comment_data = pd.read_excel(path + name)\n",
    "comment_data = shuffle(comment_data)\n",
    "encoder = LabelEncoder()\n",
    "comment_data['class'] = encoder.fit_transform(comment_data['sentiment'])\n",
    "\n",
    "test_data['sentiment'] = test_data['entlist'].apply(form_sentiment_data)\n",
    "test_data = test_data[test_data['sentiment'].apply(lambda x: bool(x))]\n",
    "test_data['sentiment'] = test_data['sentiment'].apply(rebuild_col)\n",
    "test_data = test_data[test_data['sentiment'].apply(lambda x: bool(x))]\n",
    "\n",
    "test_data['comment'] = test_data['entlist'].apply(form_comment_data)\n",
    "test_data['comment'] = test_data['comment'].apply(rebuild_col)\n",
    "\n",
    "test_data[['INN', 'Дата изменения', 'Описание', 'comment', 'sentiment']]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
